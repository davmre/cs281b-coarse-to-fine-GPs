\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\usepackage{fullpage}

\newcommand{\smallest}{\operatornamewithlimits{smallest}}

\begin{document}

\title{Coarse-to-fine Gaussian Process Regression}
\author{Yusuf Erol and Dave Moore}
\date{March 13, 2012}
\maketitle

\section{Introduction}

\section{Gaussian Processes}

Test reference: \cite{rasmussen06}. 

\section{Related Work}
As has been discussed in the earlier sections, the training requires the inversion of the Gram matrix $\mathbf{K}+\sigma^2\mathbf{I}$ which is an $N\times N$ matrix. Naive approaches for this inversion requires $O(N^3)$ operations. In the case of practical applications, N can be on the order of $10^6$ to $10^9$ and even computing and storing the Gram matrix can be prohibitive. Moreover, for big $N$, the Gram matrix has to be stored in slow memory and transferring the data from slow memory to fast memory will dominate the cost of arithmetic operations \cite{RandomizedMatrixDecompose}.  Various researchers have proposed computationally efficient techniques in order to reduce these various costs. Low rank and sparse approximations and randomized algorithms aim to reduce the cost of the matrix inversion from $O(N^3)$ to more feasible levels. There are various approaches utilizing fast matrix-vector multiplication techniques in order to reduce the costs further during training and prediction stages. Space-partitioning trees like KD-trees and cover trees have been applied as an alternative to the low rank and sparse approximations. Moreover, in order to cope with the transfer from slow memory some algorithms have been proposed that just passes over the Gram matrix once.
\subsection{Low-Rank Approximations}
The training stage requires computing the term $(\mathbf{K}+\sigma^2\mathbf{I})^{-1}\mathbf{y}$. The inversion of the Gram matrix $\mathbf{K}+\sigma^2\mathbf{I}$ or equivalently solving the linear system $(\mathbf{K}+\sigma^2\mathbf{I})\mathbf{v}=\mathbf{y}$ requires $O(N^3)$ operations. If the kernel is known to be rank q (where $q<<N$), exploiting the low-rank structure will be beneficial computationally. Even in the case of a full rank matrix, the spectrum of the eigenvalues may decay fast and this means that one can get an accurate matrix approximation in the subspace of eigenvectors with bigger eigenvalues. 

The kernel matrix $\mathbf{K}$ is a positive-semidefinite symmetric matrix, hence its eigenvalues are real and positive. Moreover, the spectral theorem states that $\mathbf{K}=\mathbf{U\Lambda U}^T$, where $\mathbf{U}$ is a unitary matrix (i.e. $\mathbf{U}\mathbf{U}^T=\mathbf{U}^T\mathbf{U}=\mathbf{I}$) and $\mathbf{\Lambda}$ is a diagonal matrix, with eigenvalues on the diagonal. The prediction at the training points is given as :
\begin{eqnarray}
\bar{f}&=&\mathbf{K}(\mathbf{K}+\sigma^2\mathbf{I})^{-1}\mathbf{y} \nonumber \\
\bar{f}&=&\mathbf{U\Lambda U}^T(\mathbf{U\Lambda U}^T+\sigma^2\mathbf{I})^{-1}\mathbf{y} \nonumber \\
\bar{f}&=& \mathbf{U\Lambda U}^T\mathbf{U}^{-T}(\mathbf{\Lambda}+\sigma^2\mathbf{I})^{-1}\mathbf{U}^{-1}\mathbf{y} \nonumber \\
\bar{f}&=& \mathbf{U\Lambda}(\mathbf{\Lambda}+\sigma^2\mathbf{I})^{-1}\mathbf{U}^{-1}\mathbf{y} 
\end{eqnarray}
Furthermore, since the eigenvectors $\left\{\mathbf{u}_i\right\}_{i=1}^N$ form a basis for the $N$ dimensional vector space, one can easily represent any vector as a linear sum of the eigenvectors. Let's assume that $\mathbf{y}=\mathbf{U}\mathbf{\gamma}$. Then,
\begin{eqnarray}
\bar{f}&=& \mathbf{U\Lambda U}^T\mathbf{U}^{-T}(\mathbf{\Lambda}+\sigma^2\mathbf{I})^{-1}\mathbf{y}  \nonumber \\
\bar{f}&=&\sum_{i=1}^n\frac{\gamma_i\lambda_i}{\lambda_i+\sigma_n^2}\mathbf{u}_i
\end{eqnarray}
Therefore, if $\frac{\lambda_i}{\lambda_i+\sigma^2}<<1$, then the component along $\mathbf{u}_i$ is negligible. In this case, the reduced-rank approximation of rank $q$ will be:
\begin{equation}
K\approx \sum_{i=1}^q \lambda_i \mathbf{u}_i \mathbf{u}_i^T; \> where \> \lambda_1\geq \lambda_2 \geq \dots \lambda_n
\end{equation}
The issue with this approach is that it requires knowing the spectrum of the eigenvalues which indeed costs $O(N^3)$.  Several approaches have been proposed in order to have a cheaper eigenvalue decomposition \cite{rasmussen06}. 

One particular approach is Nystrom method which approximates the eigenvectors of the kernel matrix by choosing a subset of data. In order to choose subsets of data several greedy algorithms have been proposed \cite{rasmussen} \cite{smolaGreedy}. A randomized matrix approximation approach is given in \cite{RandomizedMatrixDecompose} which is also a \textit{single pass} algorithm.


\subsection{Random Features}
Idea of random features is approximating the kernel using a randomized feature map $\mathbf{z}$ and expressing the problem as a linear machine problem \cite{randomfeatures}. 
\begin{equation}
k(\mathbf{x},\mathbf{x'})=<\phi(\mathbf{x})\phi(\mathbf{x'})>\approx \mathbf{z}(\mathbf{x})^T\mathbf{z}(\mathbf{x'}); \>where\> \mathbf{x} \in \mathbb{R}^d
\end{equation}
For the frequently used stationary kernels, one can utilize the Random Fourier Features. For a stationary kernel $k(\mathbf{x},\mathbf{x'})=k(\mathbf{x}-\mathbf{x'})$ and \textit{Bochner's Theorem} states that for a properly scaled kernel, the Fourier Transform of the kernel will be a probability distribution. That is; 
\begin{eqnarray}
k(\mathbf{x}-\mathbf{x'})&=&\mathcal{F}^{-1}\left\{p(\omega)\right\} \nonumber \\
&=&\int_{\mathbb{R}^d}p(\omega)e^{j\omega^T(\mathbf{x}-\mathbf{x'})}d\omega \nonumber \\
&=&\mathbb{E}[\zeta_w(\mathbf{x})\zeta_w(\mathbf{x'})^*]; \>where \> \zeta_w(\mathbf{x})=e^{j\omega^T\mathbf{x}}
\end{eqnarray}
One can express the above expectation as a monte-carlo sum. Drawing i.i.d. samples from the probability distribution one can express the kernel as:
\begin{eqnarray}
k(\mathbf{x}-\mathbf{x'})&\approx& \frac{1}{D}\sum_{i=1}^De^{j\omega_i^T\mathbf{x}}e^{-j\omega_i^T\mathbf{x'}}; \> where \> \omega_i\sim p(\omega) \> i.i.d. \nonumber \\
&\approx& \underbrace{\begin{bmatrix}
\frac{1}{\sqrt{D}} e^{jw_1^T\mathbf{x}}& \frac{1}{\sqrt{D}} e^{jw_2^T\mathbf{x}}
& \dots
& \frac{1}{\sqrt{D}} e^{jw_D^T\mathbf{x}}
\end{bmatrix} }_{\mathbf{z}^H(\mathbf{x})}
\underbrace{\begin{bmatrix}
\frac{1}{\sqrt{D}} e^{-jw_1^T\mathbf{x'}}\\ \frac{1}{\sqrt{D}} e^{-jw_2^T\mathbf{x'}}
\\ \vdots
\\ \frac{1}{\sqrt{D}} e^{-jw_D^T\mathbf{x'}}
\end{bmatrix}}_{\mathbf{z}(\mathbf{x'})}
\end{eqnarray}
\subsection{Sparse Approximations}
Sparse approximation relies on the greedy heuristic which solves the NP-hard problem of finding a vector $\mathbf{x}$ that satisfies $\left \| \mathbf{A}\mathbf{x} -\mathbf{b} \right \|_2\leq \epsilon $ with the fewest number of non-zero entries (i.e. minimum $L_1$ norm). The greedy heuristic has been investigated in \cite{natarajan}

\subsection{Transductive Learning (Bayesian Committee Machine)}

\subsection{Fast Matrix-Vector Multiplication}

\subsubsection{Space-Partitioning Trees}
\subsubsection{Multipole methods (Fast Gauss Transform)}

\subsection{Proposed Approaches}

\subsection{Datasets}

\newpage
\bibliographystyle{plain}
\bibliography{refs}

\end{document}